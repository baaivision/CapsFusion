<div align='center'>
<h1><a href="https://arxiv.org/abs/2310.20550">CapsFusion: Rethinking Image-Text Data at Scale</a></h1>

[Qiying Yu](https://yqy2001.github.io)<sup>1,2*</sup>, [Quan Sun](https://github.com/Quan-Sun)<sup>2*</sup>, [Xiaosong Zhang](https://github.com/zhangxiaosong18)<sup>2</sup>, [Yufeng Cui](https://scholar.google.com/citations?user=5Ydha2EAAAAJ&hl=en&oi=ao)<sup>2</sup>, [Fan Zhang](https://scholar.google.com/citations?user=VsJ39HMAAAAJ)<sup>2</sup><br>[Yue Cao](http://yue-cao.me)<sup>3</sup>, [Xinlong Wang](https://www.xloong.wang/)<sup>2</sup>, [Jingjing Liu](https://air.tsinghua.edu.cn/en/info/1046/1194.htm)<sup>1</sup>

<sup>1</sup> [Tsinghua, AIR](https://air.tsinghua.edu.cn/en/), <sup>2</sup> [BAAI](https://www.baai.ac.cn/english.html), <sup>3</sup> [Independent Researcher](http://yue-cao.me)<br><sup>*</sup> Equal Contribution
</div>

CapsFusion is a straightforward and scalable framework for generating high-quality captions for image-text pairs. This framework leverages large language models (LLMs) to organically incorporate the strengths of both real image-text pairs and synthetic captions generated by captioning models, to address the severe *Scalability Deficiency* and *World Knowledge Loss* issues in large multimodal models (LMMs) trained with synthetic captions.

## Examples

Examples generated by CapsFusion are provided below: ➀ real web-based captions (from LAION-2B, which are noisy), ➁ synthetic captions (from LAION-COCO, generated by BLIP, which are syntacticly and semanticly simplistic), and their corresponding ③ CapsFusion captions. 

Knowledge from raw captions (in blue) and information from synthetic captions (in yellow) are organically fused into integral CapsFusion captions. More captions and detailed analysis can be found in our [paper](https://arxiv.org/abs/2310.20550).

![](assets/capsfusion_examples.png)

Models trained on CapsFusion captions exhibit a wealth of real-world knowledge (shown in the figure below), meanwhile outperforming both real and synthetic captions in benchmark evaluations (details can be found in the [paper](https://arxiv.org/abs/2310.20550)).

![](assets/model_output_examples.png)

## Plan

Please stay tuned for upcoming releases. Thank you for your understanding.

- [ ] CapsFus-LLaMA model with inference code

- [ ] CapsFusion-10M subset: Images with Raw (from LAION-2B), Synthetic (from LAION-COCO), and CapsFusion captions

- [ ] CapsFusion-120M fullset: Image URLs with CapsFusion captions

## Reference

CapsFusion: Rethinking Image-Text Pairs at Scale -- https://arxiv.org/abs/2310.20550

```
@article{yu2023capsfusion,
  title={CapsFusion: Rethinking Image-Text Data at Scale},
  author={Yu, Qiying and Sun, Quan and Zhang, Xiaosong and Cui, Yufeng and Zhang, Fan and Cao, Yue and Wang, Xinlong and Liu, Jingjing},
  journal={arXiv preprint arXiv:2310.20550},
  year={2023}
}
```
